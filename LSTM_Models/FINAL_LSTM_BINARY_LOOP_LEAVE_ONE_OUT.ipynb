{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import preprocessing\n",
    "import tqdm\n",
    "from numpy import loadtxt\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(true,preds):\n",
    "    conf_matx = confusion_matrix(true, preds)\n",
    "    sns.heatmap(conf_matx, annot=True,annot_kws={\"size\": 12},fmt='g', cbar=False, cmap=plt.cm.Blues) #'viridis'\n",
    "    #plt.savefig('/home/jovyan/img1.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return conf_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15,5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Val'], loc='upper left')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(model_history.history['accuracy'])\n",
    "    ax.plot(model_history.history['val_accuracy'])\n",
    "    ax.set(title=model_name + ': Model Accuracy; test='+ str(np.round(model_history.history['val_accuracy'][-1], 3)),\n",
    "           ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Val'], loc='upper left')\n",
    "    #plt.savefig('/home/jovyan/img2.png')\n",
    "    plt.show()\n",
    "   \n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path_data, path_labels):\n",
    "    \n",
    "\n",
    "    feat_list = []\n",
    "\n",
    "\n",
    "    for filename in sorted(glob.glob(path_data), key=natural_keys): \n",
    "        feat_list.append(np.load(filename))\n",
    "\n",
    "    x_orig = np.reshape(feat_list, (len(feat_list),32, 64))\n",
    "\n",
    "    path = path_labels     \n",
    "    labels = pd.read_csv(path, usecols=[\"Type\", \"Category\"],\n",
    "                       sep=\",\" )\n",
    "    y_orig = np.array(labels['Category'])\n",
    "\n",
    "    return x_orig, y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_feat = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FINAL_LSTM_LEAVE_ONE_OUT/HRH_CONTROL/'\n",
    "\n",
    "l_name_feature = []\n",
    "\n",
    "for i in os.listdir(p_feat):\n",
    "    if '.' not in i:\n",
    "        l_name_feature.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cyc', 'dox', 'ket', 'olo', 'orp']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_name_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1093 samples, validate on 313 samples\n",
      "Epoch 1/300\n",
      "1093/1093 [==============================] - 4s 3ms/sample - loss: 0.6920 - accuracy: 0.6048 - val_loss: 0.5858 - val_accuracy: 0.7125\n",
      "Epoch 2/300\n",
      "1093/1093 [==============================] - 0s 235us/sample - loss: 0.6828 - accuracy: 0.6038 - val_loss: 0.5861 - val_accuracy: 0.7125\n",
      "Epoch 3/300\n",
      "1093/1093 [==============================] - 0s 247us/sample - loss: 0.6828 - accuracy: 0.6057 - val_loss: 0.5865 - val_accuracy: 0.7125\n",
      "Epoch 4/300\n",
      "1093/1093 [==============================] - 0s 216us/sample - loss: 0.6834 - accuracy: 0.6029 - val_loss: 0.5866 - val_accuracy: 0.7125\n",
      "Epoch 00004: early stopping\n",
      "247/247 [==============================] - 0s 121us/sample - loss: 0.4068 - accuracy: 1.0000\n",
      "Train on 1106 samples, validate on 353 samples\n",
      "Epoch 1/300\n",
      "1106/1106 [==============================] - 2s 2ms/sample - loss: 0.6739 - accuracy: 0.5895 - val_loss: 0.6910 - val_accuracy: 0.5184\n",
      "Epoch 2/300\n",
      "1106/1106 [==============================] - 0s 215us/sample - loss: 0.6601 - accuracy: 0.6230 - val_loss: 0.6871 - val_accuracy: 0.5297\n",
      "Epoch 3/300\n",
      "1106/1106 [==============================] - 0s 232us/sample - loss: 0.6587 - accuracy: 0.6139 - val_loss: 0.6837 - val_accuracy: 0.5609\n",
      "Epoch 4/300\n",
      "1106/1106 [==============================] - 0s 253us/sample - loss: 0.6583 - accuracy: 0.6139 - val_loss: 0.6808 - val_accuracy: 0.5637\n",
      "Epoch 5/300\n",
      "1106/1106 [==============================] - 0s 249us/sample - loss: 0.6495 - accuracy: 0.6374 - val_loss: 0.6783 - val_accuracy: 0.5807\n",
      "Epoch 6/300\n",
      "1106/1106 [==============================] - 0s 221us/sample - loss: 0.6451 - accuracy: 0.6383 - val_loss: 0.6761 - val_accuracy: 0.5949\n",
      "Epoch 7/300\n",
      "1106/1106 [==============================] - 0s 205us/sample - loss: 0.6428 - accuracy: 0.6537 - val_loss: 0.6742 - val_accuracy: 0.5977\n",
      "Epoch 8/300\n",
      "1106/1106 [==============================] - 0s 231us/sample - loss: 0.6382 - accuracy: 0.6546 - val_loss: 0.6725 - val_accuracy: 0.6062\n",
      "Epoch 9/300\n",
      "1106/1106 [==============================] - 0s 231us/sample - loss: 0.6343 - accuracy: 0.6655 - val_loss: 0.6709 - val_accuracy: 0.6062\n",
      "Epoch 10/300\n",
      "1106/1106 [==============================] - 0s 259us/sample - loss: 0.6302 - accuracy: 0.6637 - val_loss: 0.6696 - val_accuracy: 0.6119\n",
      "Epoch 11/300\n",
      "1106/1106 [==============================] - 0s 252us/sample - loss: 0.6253 - accuracy: 0.6591 - val_loss: 0.6683 - val_accuracy: 0.6062\n",
      "Epoch 12/300\n",
      "1106/1106 [==============================] - 0s 239us/sample - loss: 0.6255 - accuracy: 0.6600 - val_loss: 0.6671 - val_accuracy: 0.6062\n",
      "Epoch 13/300\n",
      "1106/1106 [==============================] - 0s 206us/sample - loss: 0.6240 - accuracy: 0.6627 - val_loss: 0.6660 - val_accuracy: 0.6062\n",
      "Epoch 14/300\n",
      "1106/1106 [==============================] - 0s 243us/sample - loss: 0.6243 - accuracy: 0.6591 - val_loss: 0.6649 - val_accuracy: 0.6062\n",
      "Epoch 15/300\n",
      "1106/1106 [==============================] - 0s 240us/sample - loss: 0.6179 - accuracy: 0.6655 - val_loss: 0.6636 - val_accuracy: 0.6062\n",
      "Epoch 16/300\n",
      "1106/1106 [==============================] - 0s 239us/sample - loss: 0.6174 - accuracy: 0.6646 - val_loss: 0.6625 - val_accuracy: 0.6062\n",
      "Epoch 17/300\n",
      "1106/1106 [==============================] - 0s 235us/sample - loss: 0.6162 - accuracy: 0.6745 - val_loss: 0.6614 - val_accuracy: 0.6062\n",
      "Epoch 18/300\n",
      "1106/1106 [==============================] - 0s 238us/sample - loss: 0.6153 - accuracy: 0.6618 - val_loss: 0.6603 - val_accuracy: 0.6062\n",
      "Epoch 19/300\n",
      "1106/1106 [==============================] - 0s 235us/sample - loss: 0.6086 - accuracy: 0.6627 - val_loss: 0.6591 - val_accuracy: 0.6091\n",
      "Epoch 20/300\n",
      "1106/1106 [==============================] - 0s 267us/sample - loss: 0.6097 - accuracy: 0.6627 - val_loss: 0.6579 - val_accuracy: 0.6091\n",
      "Epoch 21/300\n",
      "1106/1106 [==============================] - 0s 273us/sample - loss: 0.6077 - accuracy: 0.6627 - val_loss: 0.6567 - val_accuracy: 0.6062\n",
      "Epoch 22/300\n",
      "1106/1106 [==============================] - 0s 279us/sample - loss: 0.6069 - accuracy: 0.6709 - val_loss: 0.6555 - val_accuracy: 0.6062\n",
      "Epoch 23/300\n",
      "1106/1106 [==============================] - 0s 221us/sample - loss: 0.6044 - accuracy: 0.6664 - val_loss: 0.6542 - val_accuracy: 0.6062\n",
      "Epoch 24/300\n",
      "1106/1106 [==============================] - 0s 264us/sample - loss: 0.5993 - accuracy: 0.6673 - val_loss: 0.6530 - val_accuracy: 0.6091\n",
      "Epoch 25/300\n",
      "1106/1106 [==============================] - 0s 232us/sample - loss: 0.6021 - accuracy: 0.6655 - val_loss: 0.6516 - val_accuracy: 0.6119\n",
      "Epoch 26/300\n",
      "1106/1106 [==============================] - 0s 236us/sample - loss: 0.6016 - accuracy: 0.6646 - val_loss: 0.6501 - val_accuracy: 0.6176\n",
      "Epoch 27/300\n",
      "1106/1106 [==============================] - 0s 248us/sample - loss: 0.5919 - accuracy: 0.6754 - val_loss: 0.6485 - val_accuracy: 0.6147\n",
      "Epoch 28/300\n",
      "1106/1106 [==============================] - 0s 244us/sample - loss: 0.5937 - accuracy: 0.6826 - val_loss: 0.6469 - val_accuracy: 0.6147\n",
      "Epoch 29/300\n",
      "1106/1106 [==============================] - 0s 242us/sample - loss: 0.5963 - accuracy: 0.6682 - val_loss: 0.6452 - val_accuracy: 0.6147\n",
      "Epoch 30/300\n",
      "1106/1106 [==============================] - 0s 238us/sample - loss: 0.5871 - accuracy: 0.6808 - val_loss: 0.6438 - val_accuracy: 0.6176\n",
      "Epoch 31/300\n",
      "1106/1106 [==============================] - 0s 238us/sample - loss: 0.5909 - accuracy: 0.6772 - val_loss: 0.6421 - val_accuracy: 0.6232\n",
      "Epoch 32/300\n",
      "1106/1106 [==============================] - 0s 254us/sample - loss: 0.5840 - accuracy: 0.6763 - val_loss: 0.6403 - val_accuracy: 0.6232\n",
      "Epoch 33/300\n",
      "1106/1106 [==============================] - 0s 250us/sample - loss: 0.5840 - accuracy: 0.6808 - val_loss: 0.6385 - val_accuracy: 0.6232\n",
      "Epoch 34/300\n",
      "1106/1106 [==============================] - 0s 226us/sample - loss: 0.5804 - accuracy: 0.6844 - val_loss: 0.6366 - val_accuracy: 0.6204\n",
      "Epoch 35/300\n",
      "1106/1106 [==============================] - 0s 256us/sample - loss: 0.5816 - accuracy: 0.6835 - val_loss: 0.6348 - val_accuracy: 0.6232\n",
      "Epoch 36/300\n",
      "1106/1106 [==============================] - 0s 231us/sample - loss: 0.5748 - accuracy: 0.6881 - val_loss: 0.6332 - val_accuracy: 0.6289\n",
      "Epoch 37/300\n",
      "1106/1106 [==============================] - 0s 256us/sample - loss: 0.5733 - accuracy: 0.6899 - val_loss: 0.6312 - val_accuracy: 0.6289\n",
      "Epoch 38/300\n",
      "1106/1106 [==============================] - 0s 228us/sample - loss: 0.5721 - accuracy: 0.6835 - val_loss: 0.6291 - val_accuracy: 0.6289\n",
      "Epoch 39/300\n",
      "1106/1106 [==============================] - 0s 222us/sample - loss: 0.5708 - accuracy: 0.6881 - val_loss: 0.6271 - val_accuracy: 0.6289\n",
      "Epoch 40/300\n",
      "1106/1106 [==============================] - 0s 248us/sample - loss: 0.5648 - accuracy: 0.7007 - val_loss: 0.6245 - val_accuracy: 0.6261\n",
      "Epoch 41/300\n",
      "1106/1106 [==============================] - 0s 238us/sample - loss: 0.5643 - accuracy: 0.6989 - val_loss: 0.6226 - val_accuracy: 0.6289\n",
      "Epoch 42/300\n",
      "1106/1106 [==============================] - 0s 217us/sample - loss: 0.5654 - accuracy: 0.7007 - val_loss: 0.6201 - val_accuracy: 0.6317\n",
      "Epoch 43/300\n",
      "1106/1106 [==============================] - 0s 250us/sample - loss: 0.5587 - accuracy: 0.7034 - val_loss: 0.6178 - val_accuracy: 0.6402\n",
      "Epoch 44/300\n",
      "1106/1106 [==============================] - 0s 245us/sample - loss: 0.5570 - accuracy: 0.7089 - val_loss: 0.6153 - val_accuracy: 0.6459\n",
      "Epoch 45/300\n",
      "1106/1106 [==============================] - 0s 253us/sample - loss: 0.5538 - accuracy: 0.7170 - val_loss: 0.6127 - val_accuracy: 0.6487\n",
      "Epoch 46/300\n",
      "1106/1106 [==============================] - 0s 300us/sample - loss: 0.5479 - accuracy: 0.7161 - val_loss: 0.6103 - val_accuracy: 0.6487\n",
      "Epoch 47/300\n",
      "1106/1106 [==============================] - 0s 284us/sample - loss: 0.5497 - accuracy: 0.7206 - val_loss: 0.6075 - val_accuracy: 0.6544\n",
      "Epoch 48/300\n",
      "1106/1106 [==============================] - 0s 260us/sample - loss: 0.5477 - accuracy: 0.7297 - val_loss: 0.6045 - val_accuracy: 0.6516\n",
      "Epoch 49/300\n",
      "1106/1106 [==============================] - 0s 265us/sample - loss: 0.5426 - accuracy: 0.7351 - val_loss: 0.6017 - val_accuracy: 0.6516\n",
      "Epoch 50/300\n",
      "1106/1106 [==============================] - 0s 230us/sample - loss: 0.5377 - accuracy: 0.7315 - val_loss: 0.5989 - val_accuracy: 0.6544\n",
      "Epoch 51/300\n",
      "1106/1106 [==============================] - 0s 234us/sample - loss: 0.5327 - accuracy: 0.7278 - val_loss: 0.5959 - val_accuracy: 0.6601\n",
      "Epoch 52/300\n",
      "1106/1106 [==============================] - 0s 227us/sample - loss: 0.5260 - accuracy: 0.7541 - val_loss: 0.5930 - val_accuracy: 0.6657\n",
      "Epoch 53/300\n",
      "1106/1106 [==============================] - 0s 253us/sample - loss: 0.5244 - accuracy: 0.7577 - val_loss: 0.5896 - val_accuracy: 0.6714\n",
      "Epoch 54/300\n",
      "1106/1106 [==============================] - 0s 266us/sample - loss: 0.5219 - accuracy: 0.7559 - val_loss: 0.5868 - val_accuracy: 0.6799\n",
      "Epoch 55/300\n",
      "1106/1106 [==============================] - 0s 272us/sample - loss: 0.5194 - accuracy: 0.7523 - val_loss: 0.5832 - val_accuracy: 0.6742\n",
      "Epoch 56/300\n",
      "1106/1106 [==============================] - 0s 269us/sample - loss: 0.5166 - accuracy: 0.7495 - val_loss: 0.5801 - val_accuracy: 0.6771\n",
      "Epoch 57/300\n",
      "1106/1106 [==============================] - 0s 255us/sample - loss: 0.5146 - accuracy: 0.7631 - val_loss: 0.5767 - val_accuracy: 0.6884\n",
      "Epoch 58/300\n",
      "1106/1106 [==============================] - 0s 256us/sample - loss: 0.5099 - accuracy: 0.7676 - val_loss: 0.5728 - val_accuracy: 0.6884\n",
      "Epoch 59/300\n",
      "1106/1106 [==============================] - 0s 258us/sample - loss: 0.5010 - accuracy: 0.7767 - val_loss: 0.5699 - val_accuracy: 0.6969\n",
      "Epoch 60/300\n",
      "1106/1106 [==============================] - 0s 247us/sample - loss: 0.4982 - accuracy: 0.7694 - val_loss: 0.5655 - val_accuracy: 0.7110\n",
      "Epoch 61/300\n",
      "1106/1106 [==============================] - 0s 214us/sample - loss: 0.4961 - accuracy: 0.7812 - val_loss: 0.5627 - val_accuracy: 0.7139\n",
      "Epoch 62/300\n",
      "1106/1106 [==============================] - 0s 218us/sample - loss: 0.4900 - accuracy: 0.7821 - val_loss: 0.5593 - val_accuracy: 0.7139\n",
      "Epoch 63/300\n",
      "1106/1106 [==============================] - 0s 234us/sample - loss: 0.4877 - accuracy: 0.7875 - val_loss: 0.5558 - val_accuracy: 0.7167\n",
      "Epoch 64/300\n",
      "1106/1106 [==============================] - 0s 222us/sample - loss: 0.4806 - accuracy: 0.7821 - val_loss: 0.5527 - val_accuracy: 0.7252\n",
      "Epoch 65/300\n",
      "1106/1106 [==============================] - 0s 238us/sample - loss: 0.4776 - accuracy: 0.7866 - val_loss: 0.5495 - val_accuracy: 0.7280\n",
      "Epoch 66/300\n",
      "1106/1106 [==============================] - 0s 242us/sample - loss: 0.4714 - accuracy: 0.7929 - val_loss: 0.5462 - val_accuracy: 0.7337\n",
      "Epoch 67/300\n",
      "1106/1106 [==============================] - 0s 246us/sample - loss: 0.4694 - accuracy: 0.7948 - val_loss: 0.5435 - val_accuracy: 0.7365\n",
      "Epoch 68/300\n",
      "1106/1106 [==============================] - 0s 216us/sample - loss: 0.4618 - accuracy: 0.8029 - val_loss: 0.5405 - val_accuracy: 0.7365\n",
      "Epoch 69/300\n",
      "1106/1106 [==============================] - 0s 229us/sample - loss: 0.4548 - accuracy: 0.8011 - val_loss: 0.5383 - val_accuracy: 0.7422\n",
      "Epoch 70/300\n",
      "1106/1106 [==============================] - 0s 249us/sample - loss: 0.4558 - accuracy: 0.8047 - val_loss: 0.5357 - val_accuracy: 0.7394\n",
      "Epoch 71/300\n",
      "1106/1106 [==============================] - 0s 244us/sample - loss: 0.4556 - accuracy: 0.7984 - val_loss: 0.5330 - val_accuracy: 0.7479\n",
      "Epoch 72/300\n",
      "1106/1106 [==============================] - 0s 276us/sample - loss: 0.4476 - accuracy: 0.8083 - val_loss: 0.5305 - val_accuracy: 0.7507\n",
      "Epoch 73/300\n",
      "1106/1106 [==============================] - 0s 287us/sample - loss: 0.4460 - accuracy: 0.8110 - val_loss: 0.5284 - val_accuracy: 0.7535\n",
      "Epoch 74/300\n",
      "1106/1106 [==============================] - 0s 228us/sample - loss: 0.4414 - accuracy: 0.8074 - val_loss: 0.5268 - val_accuracy: 0.7535\n",
      "Epoch 75/300\n",
      "1106/1106 [==============================] - 0s 234us/sample - loss: 0.4359 - accuracy: 0.8128 - val_loss: 0.5253 - val_accuracy: 0.7535\n",
      "Epoch 76/300\n",
      "1106/1106 [==============================] - 0s 251us/sample - loss: 0.4302 - accuracy: 0.8128 - val_loss: 0.5242 - val_accuracy: 0.7535\n",
      "Epoch 77/300\n",
      "1106/1106 [==============================] - 0s 243us/sample - loss: 0.4336 - accuracy: 0.8165 - val_loss: 0.5225 - val_accuracy: 0.7479\n",
      "Epoch 78/300\n",
      "1106/1106 [==============================] - 0s 264us/sample - loss: 0.4317 - accuracy: 0.8137 - val_loss: 0.5213 - val_accuracy: 0.7422\n",
      "Epoch 79/300\n",
      "1106/1106 [==============================] - 0s 277us/sample - loss: 0.4238 - accuracy: 0.8156 - val_loss: 0.5189 - val_accuracy: 0.7365\n",
      "Epoch 80/300\n",
      "1106/1106 [==============================] - 0s 255us/sample - loss: 0.4206 - accuracy: 0.8156 - val_loss: 0.5185 - val_accuracy: 0.7365\n",
      "Epoch 81/300\n",
      "1106/1106 [==============================] - 0s 248us/sample - loss: 0.4167 - accuracy: 0.8246 - val_loss: 0.5176 - val_accuracy: 0.7365\n",
      "Epoch 82/300\n",
      "1106/1106 [==============================] - 0s 243us/sample - loss: 0.4156 - accuracy: 0.8273 - val_loss: 0.5162 - val_accuracy: 0.7394\n",
      "Epoch 83/300\n",
      "1106/1106 [==============================] - 0s 261us/sample - loss: 0.4100 - accuracy: 0.8255 - val_loss: 0.5162 - val_accuracy: 0.7422\n",
      "Epoch 84/300\n",
      "1106/1106 [==============================] - 0s 232us/sample - loss: 0.4137 - accuracy: 0.8210 - val_loss: 0.5155 - val_accuracy: 0.7422\n",
      "Epoch 85/300\n",
      "1106/1106 [==============================] - 0s 258us/sample - loss: 0.4122 - accuracy: 0.8318 - val_loss: 0.5143 - val_accuracy: 0.7450\n",
      "Epoch 86/300\n",
      "1106/1106 [==============================] - 0s 235us/sample - loss: 0.4049 - accuracy: 0.8300 - val_loss: 0.5142 - val_accuracy: 0.7450\n",
      "Epoch 87/300\n",
      "1106/1106 [==============================] - 0s 247us/sample - loss: 0.4034 - accuracy: 0.8291 - val_loss: 0.5130 - val_accuracy: 0.7450\n",
      "Epoch 88/300\n",
      "1106/1106 [==============================] - 0s 250us/sample - loss: 0.4048 - accuracy: 0.8282 - val_loss: 0.5126 - val_accuracy: 0.7450\n",
      "Epoch 89/300\n",
      "1106/1106 [==============================] - 0s 247us/sample - loss: 0.4013 - accuracy: 0.8300 - val_loss: 0.5117 - val_accuracy: 0.7450\n",
      "Epoch 90/300\n",
      "1106/1106 [==============================] - 0s 253us/sample - loss: 0.3958 - accuracy: 0.8354 - val_loss: 0.5112 - val_accuracy: 0.7450\n",
      "Epoch 91/300\n",
      "1106/1106 [==============================] - 0s 239us/sample - loss: 0.3951 - accuracy: 0.8318 - val_loss: 0.5125 - val_accuracy: 0.7479\n",
      "Epoch 92/300\n",
      "1106/1106 [==============================] - 0s 290us/sample - loss: 0.3895 - accuracy: 0.8418 - val_loss: 0.5104 - val_accuracy: 0.7507\n",
      "Epoch 93/300\n",
      "1106/1106 [==============================] - 0s 273us/sample - loss: 0.3910 - accuracy: 0.8427 - val_loss: 0.5100 - val_accuracy: 0.7535\n",
      "Epoch 94/300\n",
      "1106/1106 [==============================] - 0s 267us/sample - loss: 0.3900 - accuracy: 0.8382 - val_loss: 0.5106 - val_accuracy: 0.7535\n",
      "Epoch 95/300\n",
      "1106/1106 [==============================] - 0s 271us/sample - loss: 0.3890 - accuracy: 0.8427 - val_loss: 0.5098 - val_accuracy: 0.7564\n",
      "Epoch 96/300\n",
      "1106/1106 [==============================] - 0s 231us/sample - loss: 0.3924 - accuracy: 0.8409 - val_loss: 0.5076 - val_accuracy: 0.7535\n",
      "Epoch 97/300\n",
      "1106/1106 [==============================] - 0s 223us/sample - loss: 0.3850 - accuracy: 0.8436 - val_loss: 0.5096 - val_accuracy: 0.7564\n",
      "Epoch 98/300\n",
      "1106/1106 [==============================] - 0s 251us/sample - loss: 0.3810 - accuracy: 0.8427 - val_loss: 0.5090 - val_accuracy: 0.7564\n",
      "Epoch 99/300\n",
      "1106/1106 [==============================] - 0s 243us/sample - loss: 0.3821 - accuracy: 0.8472 - val_loss: 0.5080 - val_accuracy: 0.7564\n",
      "Epoch 00099: early stopping\n",
      "194/194 [==============================] - 0s 145us/sample - loss: 0.5840 - accuracy: 0.6753\n",
      "Train on 1061 samples, validate on 359 samples\n",
      "Epoch 1/300\n",
      "1061/1061 [==============================] - 2s 2ms/sample - loss: 0.7004 - accuracy: 0.5335 - val_loss: 0.7152 - val_accuracy: 0.5042\n",
      "Epoch 2/300\n",
      "1061/1061 [==============================] - 0s 258us/sample - loss: 0.6869 - accuracy: 0.5683 - val_loss: 0.7110 - val_accuracy: 0.5125\n",
      "Epoch 3/300\n",
      "1061/1061 [==============================] - 0s 257us/sample - loss: 0.6847 - accuracy: 0.5693 - val_loss: 0.7076 - val_accuracy: 0.5571\n",
      "Epoch 4/300\n",
      "1061/1061 [==============================] - 0s 244us/sample - loss: 0.6759 - accuracy: 0.5928 - val_loss: 0.7046 - val_accuracy: 0.5571\n",
      "Epoch 5/300\n",
      "1061/1061 [==============================] - 0s 268us/sample - loss: 0.6665 - accuracy: 0.6136 - val_loss: 0.7018 - val_accuracy: 0.5682\n",
      "Epoch 6/300\n",
      "1061/1061 [==============================] - 0s 242us/sample - loss: 0.6675 - accuracy: 0.6239 - val_loss: 0.6992 - val_accuracy: 0.5738\n",
      "Epoch 7/300\n",
      "1061/1061 [==============================] - 0s 226us/sample - loss: 0.6579 - accuracy: 0.6136 - val_loss: 0.6968 - val_accuracy: 0.5710\n",
      "Epoch 8/300\n",
      "1061/1061 [==============================] - 0s 228us/sample - loss: 0.6586 - accuracy: 0.6381 - val_loss: 0.6947 - val_accuracy: 0.5766\n",
      "Epoch 9/300\n",
      "1061/1061 [==============================] - 0s 213us/sample - loss: 0.6515 - accuracy: 0.6484 - val_loss: 0.6924 - val_accuracy: 0.5738\n",
      "Epoch 10/300\n",
      "1061/1061 [==============================] - 0s 213us/sample - loss: 0.6511 - accuracy: 0.6381 - val_loss: 0.6900 - val_accuracy: 0.5766\n",
      "Epoch 11/300\n",
      "1061/1061 [==============================] - 0s 226us/sample - loss: 0.6436 - accuracy: 0.6466 - val_loss: 0.6878 - val_accuracy: 0.5710\n",
      "Epoch 12/300\n",
      "1061/1061 [==============================] - 0s 226us/sample - loss: 0.6363 - accuracy: 0.6560 - val_loss: 0.6855 - val_accuracy: 0.5766\n",
      "Epoch 13/300\n",
      "1061/1061 [==============================] - 0s 230us/sample - loss: 0.6325 - accuracy: 0.6541 - val_loss: 0.6834 - val_accuracy: 0.5766\n",
      "Epoch 14/300\n",
      "1061/1061 [==============================] - 0s 241us/sample - loss: 0.6345 - accuracy: 0.6513 - val_loss: 0.6810 - val_accuracy: 0.5822\n",
      "Epoch 15/300\n",
      "1061/1061 [==============================] - 0s 245us/sample - loss: 0.6302 - accuracy: 0.6569 - val_loss: 0.6782 - val_accuracy: 0.5877\n",
      "Epoch 16/300\n",
      "1061/1061 [==============================] - 0s 225us/sample - loss: 0.6255 - accuracy: 0.6616 - val_loss: 0.6752 - val_accuracy: 0.5933\n",
      "Epoch 17/300\n",
      "1061/1061 [==============================] - 0s 236us/sample - loss: 0.6338 - accuracy: 0.6598 - val_loss: 0.6727 - val_accuracy: 0.5961\n",
      "Epoch 18/300\n",
      "1061/1061 [==============================] - 0s 234us/sample - loss: 0.6240 - accuracy: 0.6654 - val_loss: 0.6704 - val_accuracy: 0.5961\n",
      "Epoch 19/300\n",
      "1061/1061 [==============================] - 0s 249us/sample - loss: 0.6179 - accuracy: 0.6739 - val_loss: 0.6675 - val_accuracy: 0.5961\n",
      "Epoch 20/300\n",
      "1061/1061 [==============================] - 0s 243us/sample - loss: 0.6194 - accuracy: 0.6730 - val_loss: 0.6655 - val_accuracy: 0.5961\n",
      "Epoch 21/300\n",
      "1061/1061 [==============================] - 0s 248us/sample - loss: 0.6176 - accuracy: 0.6664 - val_loss: 0.6627 - val_accuracy: 0.5933\n",
      "Epoch 22/300\n",
      "1061/1061 [==============================] - 0s 268us/sample - loss: 0.6149 - accuracy: 0.6541 - val_loss: 0.6599 - val_accuracy: 0.5933\n",
      "Epoch 23/300\n",
      "1061/1061 [==============================] - 0s 254us/sample - loss: 0.6084 - accuracy: 0.6720 - val_loss: 0.6573 - val_accuracy: 0.5961\n",
      "Epoch 24/300\n",
      "1061/1061 [==============================] - 0s 280us/sample - loss: 0.6037 - accuracy: 0.6711 - val_loss: 0.6543 - val_accuracy: 0.5989\n",
      "Epoch 25/300\n",
      "1061/1061 [==============================] - 0s 294us/sample - loss: 0.6018 - accuracy: 0.6673 - val_loss: 0.6517 - val_accuracy: 0.5989\n",
      "Epoch 26/300\n",
      "1061/1061 [==============================] - 0s 300us/sample - loss: 0.6012 - accuracy: 0.6701 - val_loss: 0.6489 - val_accuracy: 0.5989\n",
      "Epoch 27/300\n",
      "1061/1061 [==============================] - 0s 292us/sample - loss: 0.6015 - accuracy: 0.6758 - val_loss: 0.6456 - val_accuracy: 0.6072\n",
      "Epoch 28/300\n",
      "1061/1061 [==============================] - 0s 269us/sample - loss: 0.5929 - accuracy: 0.6814 - val_loss: 0.6422 - val_accuracy: 0.6100\n",
      "Epoch 29/300\n",
      "1061/1061 [==============================] - 0s 283us/sample - loss: 0.5869 - accuracy: 0.6852 - val_loss: 0.6398 - val_accuracy: 0.6128\n",
      "Epoch 30/300\n",
      "1061/1061 [==============================] - 0s 283us/sample - loss: 0.5919 - accuracy: 0.6748 - val_loss: 0.6376 - val_accuracy: 0.6156\n",
      "Epoch 31/300\n",
      "1061/1061 [==============================] - 0s 276us/sample - loss: 0.5826 - accuracy: 0.6899 - val_loss: 0.6350 - val_accuracy: 0.6212\n",
      "Epoch 32/300\n",
      "1061/1061 [==============================] - 0s 257us/sample - loss: 0.5857 - accuracy: 0.6786 - val_loss: 0.6316 - val_accuracy: 0.6240\n",
      "Epoch 33/300\n",
      "1061/1061 [==============================] - 0s 257us/sample - loss: 0.5847 - accuracy: 0.6890 - val_loss: 0.6286 - val_accuracy: 0.6295\n",
      "Epoch 34/300\n",
      "1061/1061 [==============================] - 0s 232us/sample - loss: 0.5776 - accuracy: 0.6965 - val_loss: 0.6259 - val_accuracy: 0.6323\n",
      "Epoch 35/300\n",
      "1061/1061 [==============================] - 0s 243us/sample - loss: 0.5703 - accuracy: 0.6956 - val_loss: 0.6224 - val_accuracy: 0.6379\n",
      "Epoch 36/300\n",
      "1061/1061 [==============================] - 0s 250us/sample - loss: 0.5731 - accuracy: 0.6956 - val_loss: 0.6199 - val_accuracy: 0.6435\n",
      "Epoch 37/300\n",
      "1061/1061 [==============================] - 0s 219us/sample - loss: 0.5731 - accuracy: 0.6918 - val_loss: 0.6170 - val_accuracy: 0.6435\n",
      "Epoch 38/300\n",
      "1061/1061 [==============================] - 0s 248us/sample - loss: 0.5707 - accuracy: 0.7003 - val_loss: 0.6138 - val_accuracy: 0.6490\n",
      "Epoch 39/300\n",
      "1061/1061 [==============================] - 0s 254us/sample - loss: 0.5655 - accuracy: 0.6965 - val_loss: 0.6112 - val_accuracy: 0.6546\n",
      "Epoch 40/300\n",
      "1061/1061 [==============================] - 0s 260us/sample - loss: 0.5610 - accuracy: 0.7097 - val_loss: 0.6088 - val_accuracy: 0.6546\n",
      "Epoch 41/300\n",
      "1061/1061 [==============================] - 0s 251us/sample - loss: 0.5602 - accuracy: 0.7078 - val_loss: 0.6056 - val_accuracy: 0.6602\n",
      "Epoch 42/300\n",
      "1061/1061 [==============================] - 0s 251us/sample - loss: 0.5573 - accuracy: 0.7116 - val_loss: 0.6033 - val_accuracy: 0.6602\n",
      "Epoch 43/300\n",
      "1061/1061 [==============================] - 0s 232us/sample - loss: 0.5534 - accuracy: 0.7088 - val_loss: 0.6002 - val_accuracy: 0.6574\n",
      "Epoch 44/300\n",
      "1061/1061 [==============================] - 0s 254us/sample - loss: 0.5501 - accuracy: 0.7201 - val_loss: 0.5979 - val_accuracy: 0.6574\n",
      "Epoch 45/300\n",
      "1061/1061 [==============================] - 0s 218us/sample - loss: 0.5467 - accuracy: 0.7295 - val_loss: 0.5933 - val_accuracy: 0.6657\n",
      "Epoch 46/300\n",
      "1061/1061 [==============================] - 0s 222us/sample - loss: 0.5447 - accuracy: 0.7314 - val_loss: 0.5901 - val_accuracy: 0.6685\n",
      "Epoch 47/300\n",
      "1061/1061 [==============================] - 0s 229us/sample - loss: 0.5428 - accuracy: 0.7210 - val_loss: 0.5864 - val_accuracy: 0.6741\n",
      "Epoch 48/300\n",
      "1061/1061 [==============================] - 0s 262us/sample - loss: 0.5392 - accuracy: 0.7455 - val_loss: 0.5833 - val_accuracy: 0.6769\n",
      "Epoch 49/300\n",
      "1061/1061 [==============================] - 0s 247us/sample - loss: 0.5338 - accuracy: 0.7540 - val_loss: 0.5807 - val_accuracy: 0.6852\n",
      "Epoch 50/300\n",
      "1061/1061 [==============================] - 0s 250us/sample - loss: 0.5271 - accuracy: 0.7493 - val_loss: 0.5785 - val_accuracy: 0.6880\n",
      "Epoch 51/300\n",
      "1061/1061 [==============================] - 0s 224us/sample - loss: 0.5291 - accuracy: 0.7455 - val_loss: 0.5750 - val_accuracy: 0.6992\n",
      "Epoch 52/300\n",
      "1061/1061 [==============================] - 0s 231us/sample - loss: 0.5262 - accuracy: 0.7418 - val_loss: 0.5718 - val_accuracy: 0.7019\n",
      "Epoch 53/300\n",
      "1061/1061 [==============================] - 0s 262us/sample - loss: 0.5145 - accuracy: 0.7502 - val_loss: 0.5683 - val_accuracy: 0.7047\n",
      "Epoch 54/300\n",
      "1061/1061 [==============================] - 0s 280us/sample - loss: 0.5210 - accuracy: 0.7578 - val_loss: 0.5649 - val_accuracy: 0.7131\n",
      "Epoch 55/300\n",
      "1061/1061 [==============================] - 0s 241us/sample - loss: 0.5171 - accuracy: 0.7436 - val_loss: 0.5624 - val_accuracy: 0.7214\n",
      "Epoch 56/300\n",
      "1061/1061 [==============================] - 0s 220us/sample - loss: 0.5118 - accuracy: 0.7465 - val_loss: 0.5592 - val_accuracy: 0.7214\n",
      "Epoch 57/300\n",
      "1061/1061 [==============================] - 0s 230us/sample - loss: 0.5155 - accuracy: 0.7512 - val_loss: 0.5557 - val_accuracy: 0.7242\n",
      "Epoch 58/300\n",
      "1061/1061 [==============================] - 0s 247us/sample - loss: 0.5062 - accuracy: 0.7549 - val_loss: 0.5533 - val_accuracy: 0.7270\n",
      "Epoch 59/300\n",
      "1061/1061 [==============================] - 0s 244us/sample - loss: 0.5042 - accuracy: 0.7549 - val_loss: 0.5506 - val_accuracy: 0.7298\n",
      "Epoch 60/300\n",
      "1061/1061 [==============================] - 0s 255us/sample - loss: 0.4986 - accuracy: 0.7710 - val_loss: 0.5459 - val_accuracy: 0.7382\n",
      "Epoch 61/300\n",
      "1061/1061 [==============================] - 0s 246us/sample - loss: 0.4888 - accuracy: 0.7738 - val_loss: 0.5424 - val_accuracy: 0.7521\n",
      "Epoch 62/300\n",
      "1061/1061 [==============================] - 0s 212us/sample - loss: 0.4942 - accuracy: 0.7710 - val_loss: 0.5397 - val_accuracy: 0.7604\n",
      "Epoch 63/300\n",
      "1061/1061 [==============================] - 0s 221us/sample - loss: 0.4887 - accuracy: 0.7700 - val_loss: 0.5362 - val_accuracy: 0.7604\n",
      "Epoch 64/300\n",
      "1061/1061 [==============================] - 0s 228us/sample - loss: 0.4781 - accuracy: 0.7936 - val_loss: 0.5316 - val_accuracy: 0.7632\n",
      "Epoch 65/300\n",
      "1061/1061 [==============================] - 0s 242us/sample - loss: 0.4827 - accuracy: 0.7851 - val_loss: 0.5277 - val_accuracy: 0.7604\n",
      "Epoch 66/300\n",
      "1061/1061 [==============================] - 0s 224us/sample - loss: 0.4756 - accuracy: 0.7851 - val_loss: 0.5251 - val_accuracy: 0.7604\n",
      "Epoch 67/300\n",
      "1061/1061 [==============================] - 0s 221us/sample - loss: 0.4742 - accuracy: 0.7879 - val_loss: 0.5209 - val_accuracy: 0.7660\n",
      "Epoch 68/300\n",
      "1061/1061 [==============================] - 0s 215us/sample - loss: 0.4727 - accuracy: 0.7823 - val_loss: 0.5191 - val_accuracy: 0.7660\n",
      "Epoch 69/300\n",
      "1061/1061 [==============================] - 0s 196us/sample - loss: 0.4692 - accuracy: 0.7870 - val_loss: 0.5156 - val_accuracy: 0.7660\n",
      "Epoch 70/300\n",
      "1061/1061 [==============================] - 0s 232us/sample - loss: 0.4599 - accuracy: 0.7945 - val_loss: 0.5124 - val_accuracy: 0.7688\n",
      "Epoch 71/300\n",
      "1061/1061 [==============================] - 0s 249us/sample - loss: 0.4596 - accuracy: 0.7889 - val_loss: 0.5090 - val_accuracy: 0.7660\n",
      "Epoch 72/300\n",
      "1061/1061 [==============================] - 0s 275us/sample - loss: 0.4558 - accuracy: 0.7861 - val_loss: 0.5070 - val_accuracy: 0.7660\n",
      "Epoch 73/300\n",
      "1061/1061 [==============================] - 0s 262us/sample - loss: 0.4547 - accuracy: 0.7889 - val_loss: 0.5039 - val_accuracy: 0.7660\n",
      "Epoch 74/300\n",
      "1061/1061 [==============================] - 0s 285us/sample - loss: 0.4525 - accuracy: 0.7964 - val_loss: 0.5011 - val_accuracy: 0.7632\n",
      "Epoch 75/300\n",
      "1061/1061 [==============================] - 0s 277us/sample - loss: 0.4488 - accuracy: 0.7908 - val_loss: 0.4979 - val_accuracy: 0.7632\n",
      "Epoch 76/300\n",
      "1061/1061 [==============================] - 0s 264us/sample - loss: 0.4486 - accuracy: 0.7917 - val_loss: 0.4946 - val_accuracy: 0.7688\n",
      "Epoch 77/300\n",
      "1061/1061 [==============================] - 0s 245us/sample - loss: 0.4355 - accuracy: 0.7992 - val_loss: 0.4932 - val_accuracy: 0.7716\n",
      "Epoch 78/300\n",
      "1061/1061 [==============================] - 0s 272us/sample - loss: 0.4376 - accuracy: 0.8011 - val_loss: 0.4914 - val_accuracy: 0.7716\n",
      "Epoch 79/300\n",
      "1061/1061 [==============================] - 0s 258us/sample - loss: 0.4326 - accuracy: 0.7992 - val_loss: 0.4891 - val_accuracy: 0.7799\n",
      "Epoch 80/300\n",
      "1061/1061 [==============================] - 0s 292us/sample - loss: 0.4294 - accuracy: 0.8087 - val_loss: 0.4874 - val_accuracy: 0.7799\n",
      "Epoch 81/300\n",
      "1061/1061 [==============================] - 0s 286us/sample - loss: 0.4274 - accuracy: 0.7983 - val_loss: 0.4830 - val_accuracy: 0.7855\n",
      "Epoch 82/300\n",
      "1061/1061 [==============================] - 0s 248us/sample - loss: 0.4264 - accuracy: 0.8087 - val_loss: 0.4824 - val_accuracy: 0.7883\n",
      "Epoch 83/300\n",
      "1061/1061 [==============================] - 0s 245us/sample - loss: 0.4277 - accuracy: 0.8106 - val_loss: 0.4789 - val_accuracy: 0.7911\n",
      "Epoch 84/300\n",
      "1061/1061 [==============================] - 0s 233us/sample - loss: 0.4221 - accuracy: 0.8143 - val_loss: 0.4761 - val_accuracy: 0.7994\n",
      "Epoch 85/300\n",
      "1061/1061 [==============================] - 0s 234us/sample - loss: 0.4218 - accuracy: 0.8106 - val_loss: 0.4757 - val_accuracy: 0.7967\n",
      "Epoch 86/300\n",
      "1061/1061 [==============================] - 0s 246us/sample - loss: 0.4134 - accuracy: 0.8096 - val_loss: 0.4750 - val_accuracy: 0.7967\n",
      "Epoch 87/300\n",
      "1061/1061 [==============================] - 0s 240us/sample - loss: 0.4124 - accuracy: 0.8134 - val_loss: 0.4729 - val_accuracy: 0.7994\n",
      "Epoch 88/300\n",
      "1061/1061 [==============================] - 0s 241us/sample - loss: 0.4089 - accuracy: 0.8087 - val_loss: 0.4722 - val_accuracy: 0.7967\n",
      "Epoch 89/300\n",
      "1061/1061 [==============================] - 0s 227us/sample - loss: 0.4008 - accuracy: 0.8190 - val_loss: 0.4701 - val_accuracy: 0.7994\n",
      "Epoch 90/300\n",
      "1061/1061 [==============================] - 0s 239us/sample - loss: 0.4005 - accuracy: 0.8219 - val_loss: 0.4693 - val_accuracy: 0.8022\n",
      "Epoch 91/300\n",
      "1061/1061 [==============================] - 0s 251us/sample - loss: 0.3985 - accuracy: 0.8275 - val_loss: 0.4678 - val_accuracy: 0.8022\n",
      "Epoch 92/300\n",
      "1061/1061 [==============================] - 0s 240us/sample - loss: 0.4016 - accuracy: 0.8238 - val_loss: 0.4662 - val_accuracy: 0.8022\n",
      "Epoch 93/300\n",
      "1061/1061 [==============================] - 0s 230us/sample - loss: 0.3974 - accuracy: 0.8322 - val_loss: 0.4661 - val_accuracy: 0.8050\n",
      "Epoch 94/300\n",
      "1061/1061 [==============================] - 0s 252us/sample - loss: 0.3963 - accuracy: 0.8285 - val_loss: 0.4656 - val_accuracy: 0.8050\n",
      "Epoch 95/300\n",
      "1061/1061 [==============================] - 0s 229us/sample - loss: 0.3939 - accuracy: 0.8275 - val_loss: 0.4632 - val_accuracy: 0.8022\n",
      "Epoch 96/300\n",
      "1061/1061 [==============================] - 0s 230us/sample - loss: 0.3965 - accuracy: 0.8238 - val_loss: 0.4641 - val_accuracy: 0.8022\n",
      "Epoch 97/300\n",
      "1061/1061 [==============================] - 0s 229us/sample - loss: 0.3947 - accuracy: 0.8275 - val_loss: 0.4629 - val_accuracy: 0.8050\n",
      "Epoch 98/300\n",
      "1061/1061 [==============================] - 0s 232us/sample - loss: 0.3878 - accuracy: 0.8285 - val_loss: 0.4619 - val_accuracy: 0.8050\n",
      "Epoch 99/300\n",
      "1061/1061 [==============================] - 0s 225us/sample - loss: 0.3905 - accuracy: 0.8388 - val_loss: 0.4618 - val_accuracy: 0.8050\n",
      "Epoch 100/300\n",
      "1061/1061 [==============================] - 0s 226us/sample - loss: 0.3851 - accuracy: 0.8351 - val_loss: 0.4604 - val_accuracy: 0.8050\n",
      "Epoch 101/300\n",
      "1061/1061 [==============================] - 0s 224us/sample - loss: 0.3869 - accuracy: 0.8341 - val_loss: 0.4579 - val_accuracy: 0.8022\n",
      "Epoch 102/300\n",
      "1061/1061 [==============================] - 0s 242us/sample - loss: 0.3810 - accuracy: 0.8369 - val_loss: 0.4575 - val_accuracy: 0.8050\n",
      "Epoch 103/300\n",
      "1061/1061 [==============================] - 0s 210us/sample - loss: 0.3793 - accuracy: 0.8398 - val_loss: 0.4575 - val_accuracy: 0.8078\n",
      "Epoch 104/300\n",
      "1061/1061 [==============================] - 0s 216us/sample - loss: 0.3748 - accuracy: 0.8426 - val_loss: 0.4577 - val_accuracy: 0.8078\n",
      "Epoch 105/300\n",
      "1061/1061 [==============================] - 0s 229us/sample - loss: 0.3795 - accuracy: 0.8322 - val_loss: 0.4560 - val_accuracy: 0.8050\n",
      "Epoch 106/300\n",
      "1061/1061 [==============================] - 0s 237us/sample - loss: 0.3744 - accuracy: 0.8388 - val_loss: 0.4546 - val_accuracy: 0.8050\n",
      "Epoch 107/300\n",
      "1061/1061 [==============================] - 0s 242us/sample - loss: 0.3745 - accuracy: 0.8464 - val_loss: 0.4549 - val_accuracy: 0.8050\n",
      "Epoch 108/300\n",
      "1061/1061 [==============================] - 0s 249us/sample - loss: 0.3745 - accuracy: 0.8445 - val_loss: 0.4554 - val_accuracy: 0.8078\n",
      "Epoch 109/300\n",
      "1061/1061 [==============================] - 0s 246us/sample - loss: 0.3749 - accuracy: 0.8454 - val_loss: 0.4547 - val_accuracy: 0.8078\n",
      "Epoch 00109: early stopping\n",
      "233/233 [==============================] - 0s 124us/sample - loss: 0.4034 - accuracy: 0.8155\n",
      "Train on 1038 samples, validate on 350 samples\n",
      "Epoch 1/300\n",
      "1038/1038 [==============================] - 2s 2ms/sample - loss: 0.6828 - accuracy: 0.5780 - val_loss: 0.6801 - val_accuracy: 0.5857\n",
      "Epoch 2/300\n",
      "1038/1038 [==============================] - 0s 257us/sample - loss: 0.6687 - accuracy: 0.6146 - val_loss: 0.6777 - val_accuracy: 0.5886\n",
      "Epoch 3/300\n",
      "1038/1038 [==============================] - 0s 241us/sample - loss: 0.6679 - accuracy: 0.6060 - val_loss: 0.6755 - val_accuracy: 0.6000\n",
      "Epoch 4/300\n",
      "1038/1038 [==============================] - 0s 234us/sample - loss: 0.6685 - accuracy: 0.6002 - val_loss: 0.6734 - val_accuracy: 0.6057\n",
      "Epoch 5/300\n",
      "1038/1038 [==============================] - 0s 238us/sample - loss: 0.6636 - accuracy: 0.6108 - val_loss: 0.6714 - val_accuracy: 0.6057\n",
      "Epoch 6/300\n",
      "1038/1038 [==============================] - 0s 215us/sample - loss: 0.6575 - accuracy: 0.6089 - val_loss: 0.6694 - val_accuracy: 0.6114\n",
      "Epoch 7/300\n",
      "1038/1038 [==============================] - 0s 208us/sample - loss: 0.6586 - accuracy: 0.6156 - val_loss: 0.6677 - val_accuracy: 0.6200\n",
      "Epoch 8/300\n",
      "1038/1038 [==============================] - 0s 226us/sample - loss: 0.6513 - accuracy: 0.6233 - val_loss: 0.6658 - val_accuracy: 0.6229\n",
      "Epoch 9/300\n",
      "1038/1038 [==============================] - 0s 228us/sample - loss: 0.6447 - accuracy: 0.6378 - val_loss: 0.6641 - val_accuracy: 0.6229\n",
      "Epoch 10/300\n",
      "1038/1038 [==============================] - 0s 219us/sample - loss: 0.6504 - accuracy: 0.6329 - val_loss: 0.6621 - val_accuracy: 0.6229\n",
      "Epoch 11/300\n",
      "1038/1038 [==============================] - 0s 213us/sample - loss: 0.6495 - accuracy: 0.6195 - val_loss: 0.6604 - val_accuracy: 0.6229\n",
      "Epoch 12/300\n",
      "1038/1038 [==============================] - 0s 234us/sample - loss: 0.6421 - accuracy: 0.6493 - val_loss: 0.6585 - val_accuracy: 0.6229\n",
      "Epoch 13/300\n",
      "1038/1038 [==============================] - 0s 233us/sample - loss: 0.6395 - accuracy: 0.6426 - val_loss: 0.6569 - val_accuracy: 0.6229\n",
      "Epoch 14/300\n",
      "1038/1038 [==============================] - 0s 207us/sample - loss: 0.6379 - accuracy: 0.6339 - val_loss: 0.6551 - val_accuracy: 0.6200\n",
      "Epoch 15/300\n",
      "1038/1038 [==============================] - 0s 226us/sample - loss: 0.6321 - accuracy: 0.6455 - val_loss: 0.6534 - val_accuracy: 0.6200\n",
      "Epoch 16/300\n",
      "1038/1038 [==============================] - 0s 224us/sample - loss: 0.6286 - accuracy: 0.6513 - val_loss: 0.6515 - val_accuracy: 0.6200\n",
      "Epoch 17/300\n",
      "1038/1038 [==============================] - 0s 240us/sample - loss: 0.6232 - accuracy: 0.6686 - val_loss: 0.6498 - val_accuracy: 0.6200\n",
      "Epoch 18/300\n",
      "1038/1038 [==============================] - 0s 233us/sample - loss: 0.6234 - accuracy: 0.6532 - val_loss: 0.6477 - val_accuracy: 0.6200\n",
      "Epoch 19/300\n",
      "1038/1038 [==============================] - 0s 241us/sample - loss: 0.6205 - accuracy: 0.6667 - val_loss: 0.6457 - val_accuracy: 0.6200\n",
      "Epoch 20/300\n",
      "1038/1038 [==============================] - 0s 234us/sample - loss: 0.6207 - accuracy: 0.6657 - val_loss: 0.6438 - val_accuracy: 0.6200\n",
      "Epoch 21/300\n",
      "1038/1038 [==============================] - 0s 241us/sample - loss: 0.6136 - accuracy: 0.6647 - val_loss: 0.6417 - val_accuracy: 0.6229\n",
      "Epoch 22/300\n",
      "1038/1038 [==============================] - 0s 250us/sample - loss: 0.6149 - accuracy: 0.6590 - val_loss: 0.6398 - val_accuracy: 0.6200\n",
      "Epoch 23/300\n",
      "1038/1038 [==============================] - 0s 224us/sample - loss: 0.6102 - accuracy: 0.6638 - val_loss: 0.6379 - val_accuracy: 0.6229\n",
      "Epoch 24/300\n",
      "1038/1038 [==============================] - 0s 247us/sample - loss: 0.6071 - accuracy: 0.6811 - val_loss: 0.6358 - val_accuracy: 0.6257\n",
      "Epoch 25/300\n",
      "1038/1038 [==============================] - 0s 233us/sample - loss: 0.6097 - accuracy: 0.6753 - val_loss: 0.6338 - val_accuracy: 0.6314\n",
      "Epoch 26/300\n",
      "1038/1038 [==============================] - 0s 230us/sample - loss: 0.6025 - accuracy: 0.6898 - val_loss: 0.6316 - val_accuracy: 0.6343\n",
      "Epoch 27/300\n",
      "1038/1038 [==============================] - 0s 232us/sample - loss: 0.5989 - accuracy: 0.6763 - val_loss: 0.6294 - val_accuracy: 0.6371\n",
      "Epoch 28/300\n",
      "1038/1038 [==============================] - 0s 219us/sample - loss: 0.5988 - accuracy: 0.6811 - val_loss: 0.6272 - val_accuracy: 0.6371\n",
      "Epoch 29/300\n",
      "1038/1038 [==============================] - 0s 210us/sample - loss: 0.5970 - accuracy: 0.6879 - val_loss: 0.6249 - val_accuracy: 0.6371\n",
      "Epoch 30/300\n",
      "1038/1038 [==============================] - 0s 254us/sample - loss: 0.5960 - accuracy: 0.6792 - val_loss: 0.6227 - val_accuracy: 0.6400\n",
      "Epoch 31/300\n",
      "1038/1038 [==============================] - 0s 276us/sample - loss: 0.5877 - accuracy: 0.6917 - val_loss: 0.6202 - val_accuracy: 0.6514\n",
      "Epoch 32/300\n",
      "1038/1038 [==============================] - 0s 239us/sample - loss: 0.5876 - accuracy: 0.6917 - val_loss: 0.6180 - val_accuracy: 0.6514\n",
      "Epoch 33/300\n",
      "1038/1038 [==============================] - 0s 238us/sample - loss: 0.5815 - accuracy: 0.7013 - val_loss: 0.6152 - val_accuracy: 0.6629\n",
      "Epoch 34/300\n",
      "1038/1038 [==============================] - 0s 224us/sample - loss: 0.5805 - accuracy: 0.7071 - val_loss: 0.6128 - val_accuracy: 0.6629\n",
      "Epoch 35/300\n",
      "1038/1038 [==============================] - 0s 260us/sample - loss: 0.5772 - accuracy: 0.7081 - val_loss: 0.6104 - val_accuracy: 0.6686\n",
      "Epoch 36/300\n",
      "1038/1038 [==============================] - 0s 227us/sample - loss: 0.5720 - accuracy: 0.7168 - val_loss: 0.6079 - val_accuracy: 0.6771\n",
      "Epoch 37/300\n",
      "1038/1038 [==============================] - 0s 259us/sample - loss: 0.5721 - accuracy: 0.7004 - val_loss: 0.6052 - val_accuracy: 0.6800\n",
      "Epoch 38/300\n",
      "1038/1038 [==============================] - 0s 226us/sample - loss: 0.5687 - accuracy: 0.7177 - val_loss: 0.6024 - val_accuracy: 0.6857\n",
      "Epoch 39/300\n",
      "1038/1038 [==============================] - 0s 211us/sample - loss: 0.5629 - accuracy: 0.7283 - val_loss: 0.5998 - val_accuracy: 0.6829\n",
      "Epoch 40/300\n",
      "1038/1038 [==============================] - 0s 213us/sample - loss: 0.5598 - accuracy: 0.7293 - val_loss: 0.5972 - val_accuracy: 0.6886\n",
      "Epoch 41/300\n",
      "1038/1038 [==============================] - 0s 239us/sample - loss: 0.5567 - accuracy: 0.7303 - val_loss: 0.5942 - val_accuracy: 0.6943\n",
      "Epoch 42/300\n",
      "1038/1038 [==============================] - 0s 240us/sample - loss: 0.5506 - accuracy: 0.7245 - val_loss: 0.5911 - val_accuracy: 0.6914\n",
      "Epoch 43/300\n",
      "1038/1038 [==============================] - 0s 239us/sample - loss: 0.5520 - accuracy: 0.7274 - val_loss: 0.5879 - val_accuracy: 0.6943\n",
      "Epoch 44/300\n",
      "1038/1038 [==============================] - 0s 215us/sample - loss: 0.5496 - accuracy: 0.7293 - val_loss: 0.5850 - val_accuracy: 0.7029\n",
      "Epoch 45/300\n",
      "1038/1038 [==============================] - 0s 229us/sample - loss: 0.5425 - accuracy: 0.7447 - val_loss: 0.5820 - val_accuracy: 0.7057\n",
      "Epoch 46/300\n",
      "1038/1038 [==============================] - 0s 249us/sample - loss: 0.5404 - accuracy: 0.7457 - val_loss: 0.5789 - val_accuracy: 0.7086\n",
      "Epoch 47/300\n",
      "1038/1038 [==============================] - 0s 213us/sample - loss: 0.5367 - accuracy: 0.7380 - val_loss: 0.5757 - val_accuracy: 0.7143\n",
      "Epoch 48/300\n",
      "1038/1038 [==============================] - 0s 219us/sample - loss: 0.5312 - accuracy: 0.7418 - val_loss: 0.5725 - val_accuracy: 0.7229\n",
      "Epoch 49/300\n",
      "1038/1038 [==============================] - 0s 219us/sample - loss: 0.5267 - accuracy: 0.7514 - val_loss: 0.5692 - val_accuracy: 0.7257\n",
      "Epoch 50/300\n",
      "1038/1038 [==============================] - 0s 215us/sample - loss: 0.5194 - accuracy: 0.7630 - val_loss: 0.5659 - val_accuracy: 0.7257\n",
      "Epoch 51/300\n",
      "1038/1038 [==============================] - 0s 209us/sample - loss: 0.5186 - accuracy: 0.7543 - val_loss: 0.5625 - val_accuracy: 0.7314\n",
      "Epoch 52/300\n",
      "1038/1038 [==============================] - 0s 203us/sample - loss: 0.5123 - accuracy: 0.7640 - val_loss: 0.5590 - val_accuracy: 0.7400\n",
      "Epoch 53/300\n",
      "1038/1038 [==============================] - 0s 200us/sample - loss: 0.5088 - accuracy: 0.7755 - val_loss: 0.5553 - val_accuracy: 0.7429\n",
      "Epoch 54/300\n",
      "1038/1038 [==============================] - 0s 226us/sample - loss: 0.5033 - accuracy: 0.7688 - val_loss: 0.5518 - val_accuracy: 0.7400\n",
      "Epoch 55/300\n",
      "1038/1038 [==============================] - 0s 231us/sample - loss: 0.5042 - accuracy: 0.7707 - val_loss: 0.5481 - val_accuracy: 0.7400\n",
      "Epoch 56/300\n",
      "1038/1038 [==============================] - 0s 238us/sample - loss: 0.4945 - accuracy: 0.7842 - val_loss: 0.5445 - val_accuracy: 0.7343\n",
      "Epoch 57/300\n",
      "1038/1038 [==============================] - 0s 206us/sample - loss: 0.4926 - accuracy: 0.7765 - val_loss: 0.5409 - val_accuracy: 0.7343\n",
      "Epoch 58/300\n",
      "1038/1038 [==============================] - 0s 197us/sample - loss: 0.4870 - accuracy: 0.7775 - val_loss: 0.5368 - val_accuracy: 0.7314\n",
      "Epoch 59/300\n",
      "1038/1038 [==============================] - 0s 203us/sample - loss: 0.4801 - accuracy: 0.7919 - val_loss: 0.5328 - val_accuracy: 0.7400\n",
      "Epoch 60/300\n",
      "1038/1038 [==============================] - 0s 208us/sample - loss: 0.4767 - accuracy: 0.7996 - val_loss: 0.5290 - val_accuracy: 0.7371\n",
      "Epoch 61/300\n",
      "1038/1038 [==============================] - 0s 206us/sample - loss: 0.4706 - accuracy: 0.7929 - val_loss: 0.5253 - val_accuracy: 0.7429\n",
      "Epoch 62/300\n",
      "1038/1038 [==============================] - 0s 229us/sample - loss: 0.4672 - accuracy: 0.7958 - val_loss: 0.5214 - val_accuracy: 0.7429\n",
      "Epoch 63/300\n",
      "1038/1038 [==============================] - 0s 276us/sample - loss: 0.4662 - accuracy: 0.8025 - val_loss: 0.5175 - val_accuracy: 0.7400\n",
      "Epoch 64/300\n",
      "1038/1038 [==============================] - 0s 270us/sample - loss: 0.4584 - accuracy: 0.8064 - val_loss: 0.5135 - val_accuracy: 0.7514\n",
      "Epoch 65/300\n",
      "1038/1038 [==============================] - 0s 261us/sample - loss: 0.4544 - accuracy: 0.8092 - val_loss: 0.5098 - val_accuracy: 0.7514\n",
      "Epoch 66/300\n",
      "1038/1038 [==============================] - 0s 260us/sample - loss: 0.4548 - accuracy: 0.7977 - val_loss: 0.5058 - val_accuracy: 0.7543\n",
      "Epoch 67/300\n",
      "1038/1038 [==============================] - 0s 265us/sample - loss: 0.4432 - accuracy: 0.8208 - val_loss: 0.5023 - val_accuracy: 0.7514\n",
      "Epoch 68/300\n",
      "1038/1038 [==============================] - 0s 257us/sample - loss: 0.4440 - accuracy: 0.8073 - val_loss: 0.4986 - val_accuracy: 0.7514\n",
      "Epoch 69/300\n",
      "1038/1038 [==============================] - 0s 248us/sample - loss: 0.4335 - accuracy: 0.8150 - val_loss: 0.4942 - val_accuracy: 0.7543\n",
      "Epoch 70/300\n",
      "1038/1038 [==============================] - 0s 259us/sample - loss: 0.4271 - accuracy: 0.8304 - val_loss: 0.4907 - val_accuracy: 0.7686\n",
      "Epoch 71/300\n",
      "1038/1038 [==============================] - 0s 229us/sample - loss: 0.4310 - accuracy: 0.8179 - val_loss: 0.4875 - val_accuracy: 0.7743\n",
      "Epoch 72/300\n",
      "1038/1038 [==============================] - 0s 251us/sample - loss: 0.4237 - accuracy: 0.8208 - val_loss: 0.4838 - val_accuracy: 0.7743\n",
      "Epoch 73/300\n",
      "1038/1038 [==============================] - 0s 273us/sample - loss: 0.4213 - accuracy: 0.8112 - val_loss: 0.4802 - val_accuracy: 0.7800\n",
      "Epoch 74/300\n",
      "1038/1038 [==============================] - 0s 280us/sample - loss: 0.4133 - accuracy: 0.8285 - val_loss: 0.4774 - val_accuracy: 0.7829\n",
      "Epoch 75/300\n",
      "1038/1038 [==============================] - 0s 256us/sample - loss: 0.4132 - accuracy: 0.8285 - val_loss: 0.4742 - val_accuracy: 0.7857\n",
      "Epoch 76/300\n",
      "1038/1038 [==============================] - 0s 239us/sample - loss: 0.4142 - accuracy: 0.8121 - val_loss: 0.4713 - val_accuracy: 0.7857\n",
      "Epoch 77/300\n",
      "1038/1038 [==============================] - 0s 245us/sample - loss: 0.4087 - accuracy: 0.8227 - val_loss: 0.4682 - val_accuracy: 0.7914\n",
      "Epoch 78/300\n",
      "1038/1038 [==============================] - 0s 257us/sample - loss: 0.3991 - accuracy: 0.8333 - val_loss: 0.4656 - val_accuracy: 0.7943\n",
      "Epoch 79/300\n",
      "1038/1038 [==============================] - 0s 292us/sample - loss: 0.3992 - accuracy: 0.8295 - val_loss: 0.4631 - val_accuracy: 0.7971\n",
      "Epoch 80/300\n",
      "1038/1038 [==============================] - 0s 287us/sample - loss: 0.3969 - accuracy: 0.8372 - val_loss: 0.4602 - val_accuracy: 0.8029\n",
      "Epoch 81/300\n",
      "1038/1038 [==============================] - 0s 248us/sample - loss: 0.3918 - accuracy: 0.8266 - val_loss: 0.4576 - val_accuracy: 0.8029\n",
      "Epoch 82/300\n",
      "1038/1038 [==============================] - 0s 265us/sample - loss: 0.3863 - accuracy: 0.8401 - val_loss: 0.4548 - val_accuracy: 0.8029\n",
      "Epoch 83/300\n",
      "1038/1038 [==============================] - 0s 251us/sample - loss: 0.3847 - accuracy: 0.8362 - val_loss: 0.4530 - val_accuracy: 0.8029\n",
      "Epoch 84/300\n",
      "1038/1038 [==============================] - 0s 247us/sample - loss: 0.3845 - accuracy: 0.8353 - val_loss: 0.4503 - val_accuracy: 0.8029\n",
      "Epoch 85/300\n",
      "1038/1038 [==============================] - 0s 237us/sample - loss: 0.3812 - accuracy: 0.8353 - val_loss: 0.4483 - val_accuracy: 0.7971\n",
      "Epoch 86/300\n",
      "1038/1038 [==============================] - 0s 254us/sample - loss: 0.3762 - accuracy: 0.8459 - val_loss: 0.4466 - val_accuracy: 0.8000\n",
      "Epoch 87/300\n",
      "1038/1038 [==============================] - 0s 239us/sample - loss: 0.3773 - accuracy: 0.8372 - val_loss: 0.4441 - val_accuracy: 0.8000\n",
      "Epoch 88/300\n",
      "1038/1038 [==============================] - 0s 229us/sample - loss: 0.3709 - accuracy: 0.8468 - val_loss: 0.4424 - val_accuracy: 0.7971\n",
      "Epoch 89/300\n",
      "1038/1038 [==============================] - 0s 240us/sample - loss: 0.3734 - accuracy: 0.8459 - val_loss: 0.4405 - val_accuracy: 0.8000\n",
      "Epoch 90/300\n",
      "1038/1038 [==============================] - 0s 226us/sample - loss: 0.3684 - accuracy: 0.8410 - val_loss: 0.4381 - val_accuracy: 0.8029\n",
      "Epoch 91/300\n",
      "1038/1038 [==============================] - 0s 222us/sample - loss: 0.3643 - accuracy: 0.8459 - val_loss: 0.4361 - val_accuracy: 0.8029\n",
      "Epoch 92/300\n",
      "1038/1038 [==============================] - 0s 205us/sample - loss: 0.3624 - accuracy: 0.8497 - val_loss: 0.4347 - val_accuracy: 0.8057\n",
      "Epoch 93/300\n",
      "1038/1038 [==============================] - 0s 227us/sample - loss: 0.3586 - accuracy: 0.8526 - val_loss: 0.4330 - val_accuracy: 0.8057\n",
      "Epoch 94/300\n",
      "1038/1038 [==============================] - 0s 233us/sample - loss: 0.3572 - accuracy: 0.8468 - val_loss: 0.4314 - val_accuracy: 0.8057\n",
      "Epoch 95/300\n",
      "1038/1038 [==============================] - 0s 236us/sample - loss: 0.3618 - accuracy: 0.8449 - val_loss: 0.4303 - val_accuracy: 0.8057\n",
      "Epoch 96/300\n",
      "1038/1038 [==============================] - 0s 236us/sample - loss: 0.3563 - accuracy: 0.8574 - val_loss: 0.4285 - val_accuracy: 0.8057\n",
      "Epoch 97/300\n",
      "1038/1038 [==============================] - 0s 211us/sample - loss: 0.3558 - accuracy: 0.8478 - val_loss: 0.4277 - val_accuracy: 0.8057\n",
      "Epoch 98/300\n",
      "1038/1038 [==============================] - 0s 194us/sample - loss: 0.3497 - accuracy: 0.8584 - val_loss: 0.4262 - val_accuracy: 0.8057\n",
      "Epoch 99/300\n",
      "1038/1038 [==============================] - 0s 217us/sample - loss: 0.3465 - accuracy: 0.8603 - val_loss: 0.4253 - val_accuracy: 0.8086\n",
      "Epoch 100/300\n",
      "1038/1038 [==============================] - 0s 212us/sample - loss: 0.3454 - accuracy: 0.8584 - val_loss: 0.4238 - val_accuracy: 0.8171\n",
      "Epoch 101/300\n",
      "1038/1038 [==============================] - 0s 225us/sample - loss: 0.3466 - accuracy: 0.8565 - val_loss: 0.4226 - val_accuracy: 0.8143\n",
      "Epoch 102/300\n",
      "1038/1038 [==============================] - 0s 221us/sample - loss: 0.3443 - accuracy: 0.8565 - val_loss: 0.4219 - val_accuracy: 0.8143\n",
      "Epoch 103/300\n",
      "1038/1038 [==============================] - 0s 238us/sample - loss: 0.3438 - accuracy: 0.8603 - val_loss: 0.4210 - val_accuracy: 0.8143\n",
      "Epoch 104/300\n",
      "1038/1038 [==============================] - 0s 233us/sample - loss: 0.3374 - accuracy: 0.8603 - val_loss: 0.4196 - val_accuracy: 0.8143\n",
      "Epoch 105/300\n",
      "1038/1038 [==============================] - 0s 235us/sample - loss: 0.3396 - accuracy: 0.8516 - val_loss: 0.4186 - val_accuracy: 0.8143\n",
      "Epoch 106/300\n",
      "1038/1038 [==============================] - 0s 238us/sample - loss: 0.3353 - accuracy: 0.8613 - val_loss: 0.4175 - val_accuracy: 0.8171\n",
      "Epoch 107/300\n",
      "1038/1038 [==============================] - 0s 227us/sample - loss: 0.3412 - accuracy: 0.8536 - val_loss: 0.4168 - val_accuracy: 0.8114\n",
      "Epoch 108/300\n",
      "1038/1038 [==============================] - 0s 203us/sample - loss: 0.3396 - accuracy: 0.8584 - val_loss: 0.4161 - val_accuracy: 0.8143\n",
      "Epoch 109/300\n",
      "1038/1038 [==============================] - 0s 215us/sample - loss: 0.3305 - accuracy: 0.8584 - val_loss: 0.4151 - val_accuracy: 0.8143\n",
      "Epoch 110/300\n",
      "1038/1038 [==============================] - 0s 215us/sample - loss: 0.3297 - accuracy: 0.8574 - val_loss: 0.4145 - val_accuracy: 0.8143\n",
      "Epoch 111/300\n",
      "1038/1038 [==============================] - 0s 202us/sample - loss: 0.3284 - accuracy: 0.8622 - val_loss: 0.4137 - val_accuracy: 0.8143\n",
      "Epoch 112/300\n",
      "1038/1038 [==============================] - 0s 221us/sample - loss: 0.3292 - accuracy: 0.8584 - val_loss: 0.4129 - val_accuracy: 0.8143\n",
      "Epoch 113/300\n",
      "1038/1038 [==============================] - 0s 231us/sample - loss: 0.3272 - accuracy: 0.8651 - val_loss: 0.4120 - val_accuracy: 0.8171\n",
      "Epoch 114/300\n",
      "1038/1038 [==============================] - 0s 205us/sample - loss: 0.3257 - accuracy: 0.8603 - val_loss: 0.4117 - val_accuracy: 0.8143\n",
      "Epoch 115/300\n",
      "1038/1038 [==============================] - 0s 234us/sample - loss: 0.3285 - accuracy: 0.8622 - val_loss: 0.4111 - val_accuracy: 0.8171\n",
      "Epoch 116/300\n",
      "1038/1038 [==============================] - 0s 264us/sample - loss: 0.3249 - accuracy: 0.8661 - val_loss: 0.4107 - val_accuracy: 0.8143\n",
      "Epoch 117/300\n",
      "1038/1038 [==============================] - 0s 229us/sample - loss: 0.3232 - accuracy: 0.8651 - val_loss: 0.4097 - val_accuracy: 0.8171\n",
      "Epoch 118/300\n",
      "1038/1038 [==============================] - 0s 245us/sample - loss: 0.3263 - accuracy: 0.8661 - val_loss: 0.4092 - val_accuracy: 0.8171\n",
      "Epoch 119/300\n",
      "1038/1038 [==============================] - 0s 238us/sample - loss: 0.3225 - accuracy: 0.8719 - val_loss: 0.4090 - val_accuracy: 0.8143\n",
      "Epoch 120/300\n",
      "1038/1038 [==============================] - 0s 228us/sample - loss: 0.3196 - accuracy: 0.8622 - val_loss: 0.4083 - val_accuracy: 0.8171\n",
      "Epoch 121/300\n",
      "1038/1038 [==============================] - 0s 249us/sample - loss: 0.3178 - accuracy: 0.8680 - val_loss: 0.4089 - val_accuracy: 0.8143\n",
      "Epoch 122/300\n",
      "1038/1038 [==============================] - 0s 247us/sample - loss: 0.3162 - accuracy: 0.8699 - val_loss: 0.4087 - val_accuracy: 0.8143\n",
      "Epoch 123/300\n",
      "1038/1038 [==============================] - 0s 240us/sample - loss: 0.3187 - accuracy: 0.8699 - val_loss: 0.4075 - val_accuracy: 0.8143\n",
      "Epoch 124/300\n",
      "1038/1038 [==============================] - 0s 213us/sample - loss: 0.3163 - accuracy: 0.8709 - val_loss: 0.4070 - val_accuracy: 0.8171\n",
      "Epoch 125/300\n",
      "1038/1038 [==============================] - 0s 220us/sample - loss: 0.3116 - accuracy: 0.8728 - val_loss: 0.4072 - val_accuracy: 0.8114\n",
      "Epoch 126/300\n",
      "1038/1038 [==============================] - 0s 233us/sample - loss: 0.3150 - accuracy: 0.8651 - val_loss: 0.4070 - val_accuracy: 0.8114\n",
      "Epoch 127/300\n",
      "1038/1038 [==============================] - 0s 242us/sample - loss: 0.3128 - accuracy: 0.8690 - val_loss: 0.4067 - val_accuracy: 0.8114\n",
      "Epoch 128/300\n",
      "1038/1038 [==============================] - 0s 254us/sample - loss: 0.3107 - accuracy: 0.8699 - val_loss: 0.4065 - val_accuracy: 0.8114\n",
      "Epoch 129/300\n",
      "1038/1038 [==============================] - 0s 230us/sample - loss: 0.3077 - accuracy: 0.8776 - val_loss: 0.4066 - val_accuracy: 0.8143\n",
      "Epoch 130/300\n",
      "1038/1038 [==============================] - 0s 251us/sample - loss: 0.3076 - accuracy: 0.8767 - val_loss: 0.4061 - val_accuracy: 0.8143\n",
      "Epoch 131/300\n",
      "1038/1038 [==============================] - 0s 266us/sample - loss: 0.3078 - accuracy: 0.8719 - val_loss: 0.4052 - val_accuracy: 0.8171\n",
      "Epoch 132/300\n",
      "1038/1038 [==============================] - 0s 267us/sample - loss: 0.3105 - accuracy: 0.8680 - val_loss: 0.4048 - val_accuracy: 0.8171\n",
      "Epoch 133/300\n",
      "1038/1038 [==============================] - 0s 240us/sample - loss: 0.3073 - accuracy: 0.8738 - val_loss: 0.4051 - val_accuracy: 0.8143\n",
      "Epoch 134/300\n",
      "1038/1038 [==============================] - 0s 246us/sample - loss: 0.3018 - accuracy: 0.8738 - val_loss: 0.4048 - val_accuracy: 0.8171\n",
      "Epoch 135/300\n",
      "1038/1038 [==============================] - 0s 257us/sample - loss: 0.3047 - accuracy: 0.8757 - val_loss: 0.4051 - val_accuracy: 0.8171\n",
      "Epoch 00135: early stopping\n",
      "265/265 [==============================] - 0s 143us/sample - loss: 0.9125 - accuracy: 0.5245\n",
      "Train on 1071 samples, validate on 385 samples\n",
      "Epoch 1/300\n",
      "1071/1071 [==============================] - 2s 2ms/sample - loss: 0.6513 - accuracy: 0.6321 - val_loss: 0.6572 - val_accuracy: 0.6130\n",
      "Epoch 2/300\n",
      "1071/1071 [==============================] - 0s 253us/sample - loss: 0.6416 - accuracy: 0.6405 - val_loss: 0.6532 - val_accuracy: 0.6156\n",
      "Epoch 3/300\n",
      "1071/1071 [==============================] - 0s 285us/sample - loss: 0.6433 - accuracy: 0.6368 - val_loss: 0.6495 - val_accuracy: 0.6130\n",
      "Epoch 4/300\n",
      "1071/1071 [==============================] - 0s 306us/sample - loss: 0.6357 - accuracy: 0.6405 - val_loss: 0.6456 - val_accuracy: 0.6182\n",
      "Epoch 5/300\n",
      "1071/1071 [==============================] - 0s 257us/sample - loss: 0.6390 - accuracy: 0.6331 - val_loss: 0.6420 - val_accuracy: 0.6234\n",
      "Epoch 6/300\n",
      "1071/1071 [==============================] - 0s 220us/sample - loss: 0.6308 - accuracy: 0.6452 - val_loss: 0.6383 - val_accuracy: 0.6234\n",
      "Epoch 7/300\n",
      "1071/1071 [==============================] - 0s 234us/sample - loss: 0.6263 - accuracy: 0.6480 - val_loss: 0.6344 - val_accuracy: 0.6286\n",
      "Epoch 8/300\n",
      "1071/1071 [==============================] - 0s 235us/sample - loss: 0.6223 - accuracy: 0.6545 - val_loss: 0.6311 - val_accuracy: 0.6312\n",
      "Epoch 9/300\n",
      "1071/1071 [==============================] - 0s 220us/sample - loss: 0.6134 - accuracy: 0.6573 - val_loss: 0.6274 - val_accuracy: 0.6364\n",
      "Epoch 10/300\n",
      "1071/1071 [==============================] - 0s 241us/sample - loss: 0.6074 - accuracy: 0.6779 - val_loss: 0.6240 - val_accuracy: 0.6364\n",
      "Epoch 11/300\n",
      "1071/1071 [==============================] - 0s 236us/sample - loss: 0.6090 - accuracy: 0.6620 - val_loss: 0.6206 - val_accuracy: 0.6390\n",
      "Epoch 12/300\n",
      "1071/1071 [==============================] - 0s 236us/sample - loss: 0.6071 - accuracy: 0.6667 - val_loss: 0.6167 - val_accuracy: 0.6390\n",
      "Epoch 13/300\n",
      "1071/1071 [==============================] - 0s 235us/sample - loss: 0.5991 - accuracy: 0.6732 - val_loss: 0.6133 - val_accuracy: 0.6416\n",
      "Epoch 14/300\n",
      "1071/1071 [==============================] - 0s 258us/sample - loss: 0.6001 - accuracy: 0.6713 - val_loss: 0.6094 - val_accuracy: 0.6442\n",
      "Epoch 15/300\n",
      "1071/1071 [==============================] - 0s 234us/sample - loss: 0.5975 - accuracy: 0.6769 - val_loss: 0.6060 - val_accuracy: 0.6494\n",
      "Epoch 16/300\n",
      "1071/1071 [==============================] - 0s 229us/sample - loss: 0.5957 - accuracy: 0.6769 - val_loss: 0.6025 - val_accuracy: 0.6519\n",
      "Epoch 17/300\n",
      "1071/1071 [==============================] - 0s 218us/sample - loss: 0.5876 - accuracy: 0.6807 - val_loss: 0.5991 - val_accuracy: 0.6571\n",
      "Epoch 18/300\n",
      "1071/1071 [==============================] - 0s 236us/sample - loss: 0.5861 - accuracy: 0.6928 - val_loss: 0.5955 - val_accuracy: 0.6571\n",
      "Epoch 19/300\n",
      "1071/1071 [==============================] - 0s 244us/sample - loss: 0.5809 - accuracy: 0.6909 - val_loss: 0.5919 - val_accuracy: 0.6701\n",
      "Epoch 20/300\n",
      "1071/1071 [==============================] - 0s 239us/sample - loss: 0.5788 - accuracy: 0.7040 - val_loss: 0.5884 - val_accuracy: 0.6701\n",
      "Epoch 21/300\n",
      "1071/1071 [==============================] - 0s 247us/sample - loss: 0.5746 - accuracy: 0.7040 - val_loss: 0.5848 - val_accuracy: 0.6727\n",
      "Epoch 22/300\n",
      "1071/1071 [==============================] - 0s 206us/sample - loss: 0.5699 - accuracy: 0.7096 - val_loss: 0.5813 - val_accuracy: 0.6779\n",
      "Epoch 23/300\n",
      "1071/1071 [==============================] - 0s 208us/sample - loss: 0.5689 - accuracy: 0.7096 - val_loss: 0.5775 - val_accuracy: 0.6805\n",
      "Epoch 24/300\n",
      "1071/1071 [==============================] - 0s 225us/sample - loss: 0.5653 - accuracy: 0.7031 - val_loss: 0.5740 - val_accuracy: 0.6805\n",
      "Epoch 25/300\n",
      "1071/1071 [==============================] - 0s 222us/sample - loss: 0.5626 - accuracy: 0.7040 - val_loss: 0.5705 - val_accuracy: 0.6857\n",
      "Epoch 26/300\n",
      "1071/1071 [==============================] - 0s 223us/sample - loss: 0.5581 - accuracy: 0.7106 - val_loss: 0.5669 - val_accuracy: 0.6961\n",
      "Epoch 27/300\n",
      "1071/1071 [==============================] - 0s 241us/sample - loss: 0.5453 - accuracy: 0.7246 - val_loss: 0.5638 - val_accuracy: 0.7039\n",
      "Epoch 28/300\n",
      "1071/1071 [==============================] - 0s 233us/sample - loss: 0.5529 - accuracy: 0.7115 - val_loss: 0.5596 - val_accuracy: 0.7143\n",
      "Epoch 29/300\n",
      "1071/1071 [==============================] - 0s 239us/sample - loss: 0.5428 - accuracy: 0.7404 - val_loss: 0.5564 - val_accuracy: 0.7247\n",
      "Epoch 30/300\n",
      "1071/1071 [==============================] - 0s 246us/sample - loss: 0.5410 - accuracy: 0.7311 - val_loss: 0.5527 - val_accuracy: 0.7299\n",
      "Epoch 31/300\n",
      "1071/1071 [==============================] - 0s 239us/sample - loss: 0.5383 - accuracy: 0.7236 - val_loss: 0.5489 - val_accuracy: 0.7481\n",
      "Epoch 32/300\n",
      "1071/1071 [==============================] - 0s 249us/sample - loss: 0.5367 - accuracy: 0.7376 - val_loss: 0.5452 - val_accuracy: 0.7506\n",
      "Epoch 33/300\n",
      "1071/1071 [==============================] - 0s 244us/sample - loss: 0.5277 - accuracy: 0.7283 - val_loss: 0.5413 - val_accuracy: 0.7558\n",
      "Epoch 34/300\n",
      "1071/1071 [==============================] - 0s 218us/sample - loss: 0.5267 - accuracy: 0.7414 - val_loss: 0.5376 - val_accuracy: 0.7636\n",
      "Epoch 35/300\n",
      "1071/1071 [==============================] - 0s 236us/sample - loss: 0.5181 - accuracy: 0.7544 - val_loss: 0.5337 - val_accuracy: 0.7714\n",
      "Epoch 36/300\n",
      "1071/1071 [==============================] - 0s 250us/sample - loss: 0.5137 - accuracy: 0.7675 - val_loss: 0.5296 - val_accuracy: 0.7766\n",
      "Epoch 37/300\n",
      "1071/1071 [==============================] - 0s 232us/sample - loss: 0.5199 - accuracy: 0.7554 - val_loss: 0.5258 - val_accuracy: 0.7792\n",
      "Epoch 38/300\n",
      "1071/1071 [==============================] - 0s 234us/sample - loss: 0.5113 - accuracy: 0.7600 - val_loss: 0.5217 - val_accuracy: 0.7792\n",
      "Epoch 39/300\n",
      "1071/1071 [==============================] - 0s 221us/sample - loss: 0.5058 - accuracy: 0.7694 - val_loss: 0.5180 - val_accuracy: 0.7844\n",
      "Epoch 40/300\n",
      "1071/1071 [==============================] - 0s 226us/sample - loss: 0.4975 - accuracy: 0.7610 - val_loss: 0.5143 - val_accuracy: 0.7896\n",
      "Epoch 41/300\n",
      "1071/1071 [==============================] - 0s 227us/sample - loss: 0.4947 - accuracy: 0.7656 - val_loss: 0.5100 - val_accuracy: 0.7974\n",
      "Epoch 42/300\n",
      "1071/1071 [==============================] - 0s 251us/sample - loss: 0.4896 - accuracy: 0.7740 - val_loss: 0.5061 - val_accuracy: 0.7948\n",
      "Epoch 43/300\n",
      "1071/1071 [==============================] - 0s 242us/sample - loss: 0.4868 - accuracy: 0.7712 - val_loss: 0.5016 - val_accuracy: 0.7922\n",
      "Epoch 44/300\n",
      "1071/1071 [==============================] - 0s 250us/sample - loss: 0.4798 - accuracy: 0.7852 - val_loss: 0.4976 - val_accuracy: 0.8000\n",
      "Epoch 45/300\n",
      "1071/1071 [==============================] - 0s 229us/sample - loss: 0.4788 - accuracy: 0.7974 - val_loss: 0.4929 - val_accuracy: 0.8052\n",
      "Epoch 46/300\n",
      "1071/1071 [==============================] - 0s 219us/sample - loss: 0.4696 - accuracy: 0.7937 - val_loss: 0.4885 - val_accuracy: 0.8000\n",
      "Epoch 47/300\n",
      "1071/1071 [==============================] - 0s 217us/sample - loss: 0.4626 - accuracy: 0.8039 - val_loss: 0.4845 - val_accuracy: 0.8000\n",
      "Epoch 48/300\n",
      "1071/1071 [==============================] - 0s 200us/sample - loss: 0.4605 - accuracy: 0.8030 - val_loss: 0.4797 - val_accuracy: 0.8026\n",
      "Epoch 49/300\n",
      "1071/1071 [==============================] - 0s 234us/sample - loss: 0.4544 - accuracy: 0.8002 - val_loss: 0.4759 - val_accuracy: 0.8078\n",
      "Epoch 50/300\n",
      "1071/1071 [==============================] - 0s 256us/sample - loss: 0.4537 - accuracy: 0.8021 - val_loss: 0.4708 - val_accuracy: 0.8052\n",
      "Epoch 51/300\n",
      "1071/1071 [==============================] - 0s 270us/sample - loss: 0.4428 - accuracy: 0.8049 - val_loss: 0.4663 - val_accuracy: 0.8130\n",
      "Epoch 52/300\n",
      "1071/1071 [==============================] - 0s 271us/sample - loss: 0.4349 - accuracy: 0.8179 - val_loss: 0.4618 - val_accuracy: 0.8104\n",
      "Epoch 53/300\n",
      "1071/1071 [==============================] - 0s 220us/sample - loss: 0.4360 - accuracy: 0.8217 - val_loss: 0.4573 - val_accuracy: 0.8156\n",
      "Epoch 54/300\n",
      "1071/1071 [==============================] - 0s 222us/sample - loss: 0.4291 - accuracy: 0.8198 - val_loss: 0.4529 - val_accuracy: 0.8208\n",
      "Epoch 55/300\n",
      "1071/1071 [==============================] - 0s 219us/sample - loss: 0.4215 - accuracy: 0.8347 - val_loss: 0.4484 - val_accuracy: 0.8208\n",
      "Epoch 56/300\n",
      "1071/1071 [==============================] - 0s 231us/sample - loss: 0.4158 - accuracy: 0.8357 - val_loss: 0.4436 - val_accuracy: 0.8286\n",
      "Epoch 57/300\n",
      "1071/1071 [==============================] - 0s 284us/sample - loss: 0.4088 - accuracy: 0.8319 - val_loss: 0.4394 - val_accuracy: 0.8286\n",
      "Epoch 58/300\n",
      "1071/1071 [==============================] - 0s 271us/sample - loss: 0.4038 - accuracy: 0.8319 - val_loss: 0.4343 - val_accuracy: 0.8390\n",
      "Epoch 59/300\n",
      "1071/1071 [==============================] - 0s 265us/sample - loss: 0.3962 - accuracy: 0.8459 - val_loss: 0.4306 - val_accuracy: 0.8390\n",
      "Epoch 60/300\n",
      "1071/1071 [==============================] - 0s 281us/sample - loss: 0.3921 - accuracy: 0.8422 - val_loss: 0.4259 - val_accuracy: 0.8416\n",
      "Epoch 61/300\n",
      "1071/1071 [==============================] - 0s 286us/sample - loss: 0.3862 - accuracy: 0.8478 - val_loss: 0.4223 - val_accuracy: 0.8442\n",
      "Epoch 62/300\n",
      "1071/1071 [==============================] - 0s 243us/sample - loss: 0.3810 - accuracy: 0.8590 - val_loss: 0.4187 - val_accuracy: 0.8545\n",
      "Epoch 63/300\n",
      "1071/1071 [==============================] - 0s 264us/sample - loss: 0.3741 - accuracy: 0.8487 - val_loss: 0.4157 - val_accuracy: 0.8597\n",
      "Epoch 64/300\n",
      "1071/1071 [==============================] - 0s 295us/sample - loss: 0.3689 - accuracy: 0.8599 - val_loss: 0.4113 - val_accuracy: 0.8623\n",
      "Epoch 65/300\n",
      "1071/1071 [==============================] - 0s 274us/sample - loss: 0.3669 - accuracy: 0.8590 - val_loss: 0.4075 - val_accuracy: 0.8571\n",
      "Epoch 66/300\n",
      "1071/1071 [==============================] - 0s 229us/sample - loss: 0.3599 - accuracy: 0.8627 - val_loss: 0.4047 - val_accuracy: 0.8545\n",
      "Epoch 67/300\n",
      "1071/1071 [==============================] - 0s 225us/sample - loss: 0.3514 - accuracy: 0.8637 - val_loss: 0.4011 - val_accuracy: 0.8494\n",
      "Epoch 68/300\n",
      "1071/1071 [==============================] - 0s 239us/sample - loss: 0.3464 - accuracy: 0.8683 - val_loss: 0.3989 - val_accuracy: 0.8519\n",
      "Epoch 69/300\n",
      "1071/1071 [==============================] - 0s 259us/sample - loss: 0.3477 - accuracy: 0.8609 - val_loss: 0.3967 - val_accuracy: 0.8545\n",
      "Epoch 70/300\n",
      "1071/1071 [==============================] - 0s 248us/sample - loss: 0.3376 - accuracy: 0.8702 - val_loss: 0.3944 - val_accuracy: 0.8545\n",
      "Epoch 71/300\n",
      "1071/1071 [==============================] - 0s 236us/sample - loss: 0.3361 - accuracy: 0.8749 - val_loss: 0.3916 - val_accuracy: 0.8545\n",
      "Epoch 72/300\n",
      "1071/1071 [==============================] - 0s 230us/sample - loss: 0.3295 - accuracy: 0.8683 - val_loss: 0.3892 - val_accuracy: 0.8571\n",
      "Epoch 73/300\n",
      "1071/1071 [==============================] - 0s 253us/sample - loss: 0.3292 - accuracy: 0.8758 - val_loss: 0.3877 - val_accuracy: 0.8597\n",
      "Epoch 74/300\n",
      "1071/1071 [==============================] - 0s 245us/sample - loss: 0.3204 - accuracy: 0.8758 - val_loss: 0.3859 - val_accuracy: 0.8623\n",
      "Epoch 75/300\n",
      "1071/1071 [==============================] - 0s 245us/sample - loss: 0.3162 - accuracy: 0.8842 - val_loss: 0.3840 - val_accuracy: 0.8545\n",
      "Epoch 76/300\n",
      "1071/1071 [==============================] - 0s 282us/sample - loss: 0.3169 - accuracy: 0.8758 - val_loss: 0.3823 - val_accuracy: 0.8545\n",
      "Epoch 77/300\n",
      "1071/1071 [==============================] - 0s 237us/sample - loss: 0.3124 - accuracy: 0.8814 - val_loss: 0.3813 - val_accuracy: 0.8519\n",
      "Epoch 78/300\n",
      "1071/1071 [==============================] - 0s 260us/sample - loss: 0.3121 - accuracy: 0.8852 - val_loss: 0.3804 - val_accuracy: 0.8545\n",
      "Epoch 79/300\n",
      "1071/1071 [==============================] - 0s 262us/sample - loss: 0.3093 - accuracy: 0.8842 - val_loss: 0.3784 - val_accuracy: 0.8519\n",
      "Epoch 80/300\n",
      "1071/1071 [==============================] - 0s 239us/sample - loss: 0.3051 - accuracy: 0.8852 - val_loss: 0.3776 - val_accuracy: 0.8519\n",
      "Epoch 81/300\n",
      "1071/1071 [==============================] - 0s 218us/sample - loss: 0.2970 - accuracy: 0.8852 - val_loss: 0.3770 - val_accuracy: 0.8545\n",
      "Epoch 82/300\n",
      "1071/1071 [==============================] - 0s 248us/sample - loss: 0.2993 - accuracy: 0.8824 - val_loss: 0.3759 - val_accuracy: 0.8545\n",
      "Epoch 83/300\n",
      "1071/1071 [==============================] - 0s 259us/sample - loss: 0.2973 - accuracy: 0.8917 - val_loss: 0.3754 - val_accuracy: 0.8545\n",
      "Epoch 84/300\n",
      "1071/1071 [==============================] - 0s 268us/sample - loss: 0.2923 - accuracy: 0.8870 - val_loss: 0.3741 - val_accuracy: 0.8571\n",
      "Epoch 85/300\n",
      "1071/1071 [==============================] - 0s 268us/sample - loss: 0.2942 - accuracy: 0.8926 - val_loss: 0.3738 - val_accuracy: 0.8571\n",
      "Epoch 86/300\n",
      "1071/1071 [==============================] - 0s 282us/sample - loss: 0.2814 - accuracy: 0.8945 - val_loss: 0.3729 - val_accuracy: 0.8519\n",
      "Epoch 87/300\n",
      "1071/1071 [==============================] - 0s 251us/sample - loss: 0.2824 - accuracy: 0.8964 - val_loss: 0.3723 - val_accuracy: 0.8519\n",
      "Epoch 88/300\n",
      "1071/1071 [==============================] - 0s 238us/sample - loss: 0.2866 - accuracy: 0.8945 - val_loss: 0.3719 - val_accuracy: 0.8519\n",
      "Epoch 89/300\n",
      "1071/1071 [==============================] - 0s 253us/sample - loss: 0.2775 - accuracy: 0.8954 - val_loss: 0.3717 - val_accuracy: 0.8519\n",
      "Epoch 90/300\n",
      "1071/1071 [==============================] - 0s 247us/sample - loss: 0.2793 - accuracy: 0.8992 - val_loss: 0.3710 - val_accuracy: 0.8571\n",
      "Epoch 91/300\n",
      "1071/1071 [==============================] - 0s 230us/sample - loss: 0.2774 - accuracy: 0.8973 - val_loss: 0.3703 - val_accuracy: 0.8571\n",
      "Epoch 92/300\n",
      "1071/1071 [==============================] - 0s 244us/sample - loss: 0.2752 - accuracy: 0.9020 - val_loss: 0.3701 - val_accuracy: 0.8571\n",
      "Epoch 93/300\n",
      "1071/1071 [==============================] - 0s 252us/sample - loss: 0.2696 - accuracy: 0.9048 - val_loss: 0.3701 - val_accuracy: 0.8571\n",
      "Epoch 94/300\n",
      "1071/1071 [==============================] - 0s 248us/sample - loss: 0.2696 - accuracy: 0.9010 - val_loss: 0.3697 - val_accuracy: 0.8597\n",
      "Epoch 95/300\n",
      "1071/1071 [==============================] - 0s 277us/sample - loss: 0.2703 - accuracy: 0.9001 - val_loss: 0.3700 - val_accuracy: 0.8623\n",
      "Epoch 96/300\n",
      "1071/1071 [==============================] - 0s 254us/sample - loss: 0.2693 - accuracy: 0.9038 - val_loss: 0.3694 - val_accuracy: 0.8649\n",
      "Epoch 97/300\n",
      "1071/1071 [==============================] - 0s 238us/sample - loss: 0.2658 - accuracy: 0.9048 - val_loss: 0.3688 - val_accuracy: 0.8649\n",
      "Epoch 98/300\n",
      "1071/1071 [==============================] - 0s 250us/sample - loss: 0.2612 - accuracy: 0.9048 - val_loss: 0.3688 - val_accuracy: 0.8649\n",
      "Epoch 99/300\n",
      "1071/1071 [==============================] - 0s 257us/sample - loss: 0.2615 - accuracy: 0.9048 - val_loss: 0.3680 - val_accuracy: 0.8623\n",
      "Epoch 100/300\n",
      "1071/1071 [==============================] - 0s 261us/sample - loss: 0.2627 - accuracy: 0.9066 - val_loss: 0.3682 - val_accuracy: 0.8649\n",
      "Epoch 101/300\n",
      "1071/1071 [==============================] - 0s 262us/sample - loss: 0.2586 - accuracy: 0.9085 - val_loss: 0.3682 - val_accuracy: 0.8649\n",
      "Epoch 102/300\n",
      "1071/1071 [==============================] - 0s 236us/sample - loss: 0.2602 - accuracy: 0.9094 - val_loss: 0.3683 - val_accuracy: 0.8649\n",
      "Epoch 00102: early stopping\n",
      "197/197 [==============================] - 0s 138us/sample - loss: 0.7932 - accuracy: 0.6345\n"
     ]
    }
   ],
   "source": [
    "for i in l_name_feature:\n",
    "\n",
    "    p_feat = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FINAL_LSTM_LEAVE_ONE_OUT/HRH_CONTROL/{}/'.format(i)\n",
    "\n",
    "    train_data = p_feat + 'features_train/*.npy'\n",
    "    train_lab= p_feat + 'lstm_train.csv'\n",
    "\n",
    "    val_data = p_feat + 'features_validation/*.npy'\n",
    "    val_lab = p_feat + 'lstm_validation.csv'\n",
    "\n",
    "\n",
    "    test_data= p_feat + 'features_test/*.npy'\n",
    "    test_lab= p_feat + 'lstm_test.csv'\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "\n",
    "    x_train, y_train = loadImages(train_data, train_lab)\n",
    "\n",
    "\n",
    "    x_val, y_val = loadImages(val_data, val_lab)\n",
    "\n",
    "\n",
    "    x_test, y_test = loadImages(test_data, test_lab)\n",
    "\n",
    "\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(y_train),y_train)\n",
    "\n",
    "\n",
    "    m = Sequential()\n",
    "    m.add(LSTM(64, input_shape = (x_train.shape[1],x_train.shape[2])))\n",
    "    m.add(Dropout(0.2))\n",
    "    m.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=1e-5)\n",
    "\n",
    "    m.compile(loss= keras.losses.binary_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "    epochs = 300\n",
    "\n",
    "    m4_h = m.fit(x_train,y_train,\n",
    "\n",
    "                     callbacks = [es],\n",
    "\n",
    "                    epochs=epochs,\n",
    "                    validation_data = (x_val,y_val), \n",
    "\n",
    "                    class_weight = weights)\n",
    "\n",
    "    scores = m.evaluate(x_test, y_test)\n",
    "    tot_score.append(scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100.0, 'cyc'),\n",
       " (67.52577424049377, 'dox'),\n",
       " (81.5450668334961, 'ket'),\n",
       " (52.45283246040344, 'olo'),\n",
       " (63.45177888870239, 'orp')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tot_score, l_name_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = m.predict(x_test)\n",
    "\n",
    "t= []\n",
    "for i in (test_preds):\n",
    "    if i > 0.5:\n",
    "        t.append(1)\n",
    "    else:\n",
    "        t.append(0)\n",
    "\n",
    "draw_confusion_matrix(y_test, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
